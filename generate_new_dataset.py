from datasets import load_dataset
from pathlib import Path

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 0. Fileâ€‘paths
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
INPUT_FILE  = Path("data/multitask/input-gemini.txt")
OUTPUT_FILE = INPUT_FILE          # append inâ€‘place
OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. Target sizes
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TARGET_QUOTA = dict(CODE=2000, HI_GK=1000, UPSC=1000)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. 100â€¯% parquet / arrow sources
#    (no remote scripts â‡’ no trust_remote_code headaches)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
HF_DATASETS = [
    # 2â€¯000 random code snippets (Python & friends, 5â€¯M rows total)
    ("codeparrot/codeparrot-clean",           # id
     "train",                                 # split
     "CODE",                                  # tag
     "content",                               # question field
     None,                                    # answer field
     None,                                    # filterâ€‘fn
     dict(streaming=True)),                   # load_dataset kwargs

    # 1â€¯000 Hindi generalâ€‘knowledge / history Qâ€‘A
    ("kaifahmad/indian-history-hindi-QA-3.4k",
     "train",
     "HI_GK",
     "Question",
     "Answer",
     None,
     {}),

    # 1â€¯000 UPSC policy / exam FAQs (310 rows Ã— repeat until 1000)
    ("prnv19/UPSC_FAQ",
     "train",
     "UPSC",
     "Question",
     "Answer",
     None,
     {}),
]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. Helpers
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_current_counts():
    counts  = {k: 0 for k in TARGET_QUOTA}
    seen_qs = set()
    if INPUT_FILE.exists():
        current_tag = None
        for line in INPUT_FILE.read_text(encoding="utfâ€‘8").splitlines():
            line = line.strip()
            if line.startswith("[") and line.endswith("]"):
                current_tag = line[1:-1]
            elif line.startswith("Q: ") and current_tag in counts:
                q = line[3:]
                if q not in seen_qs:
                    counts[current_tag] += 1
                    seen_qs.add(q)
    return counts, seen_qs


def write_block(fp, tag, q, a=""):
    """Uniform format for every tag."""
    fp.write(f"[{tag}]\nQ: {q.strip()}\nA: {a.strip()}\n\n")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. Main routine
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def generate_missing():
    current_counts, seen = get_current_counts()
    print("ðŸ“Š Existing counts:", current_counts)

    with OUTPUT_FILE.open("a", encoding="utfâ€‘8") as out:
        for ds_id, split, tag, qk, ak, filt_fn, ld_kwargs in HF_DATASETS:
            need = TARGET_QUOTA[tag] - current_counts[tag]
            if need <= 0:
                print(f"âœ… {tag} already satisfied")
                continue

            print(f"ðŸ”¹ Fetching {tag} â†’ need {need}")
            try:
                ds = load_dataset(ds_id, split=split, **ld_kwargs)
            except Exception as e:
                print(f"âš ï¸  Could not load {ds_id}: {e}")
                continue

            added = 0
            for row in ds:
                if added >= need:
                    break
                if filt_fn and not filt_fn(row):
                    continue

                q = row.get(qk, "").strip()
                if not q or q in seen:
                    continue

                a = ""
                if ak:
                    a = row.get(ak, "")
                    if isinstance(a, list):
                        a = a[0] if a else ""

                write_block(out, tag, q, a)
                seen.add(q)
                added += 1

            current_counts[tag] += added
            print(f"   â†’ added {added} new rows for {tag}")

    print("\nâœ…  All done â€“ data appended to", OUTPUT_FILE)


if __name__ == "__main__":
    generate_missing()


# #!/usr/bin/env python3
# """
# generate_dataset_quota.py  â€”  DEDUP EDITION
# Build a BabyGPT training file with exact perâ€‘tag quotas
# while ensuring every question (or code prompt) is unique.
# """

# from pathlib import Path
# from collections import defaultdict
# from random import shuffle, choice, randint
# from datasets import load_dataset
# import csv
# from tqdm import tqdm

# # â”€â”€â”€â”€â”€ 1) Desired quotas â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# QUOTA = {
#     "EN_GK"   : 10_000,
#     "CODE"    : 2_500,
#     "HI_GK"   : 1_000,
#     "UPSC"    : 1_000,
#     "WHATSAPP":   500,
#     "YOUTUBE" :   500,
#     "RESUME"  :   500,
#     "EMAIL"   :   500,
#     "STARTUP" :   500,
#     "ENGLISH" :   500,
# }

# # â”€â”€â”€â”€â”€ 2) HuggingFace sources (tested, public) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# HF_SOURCES = [
#     ("squad", "train",      "EN_GK", "question", "answers.text", None),
#     ("squad", "validation", "EN_GK", "question", "answers.text", None),

#     ("mbpp", "train", "CODE", "text", None, None),

#     ("ai4bharat/indicqa", "train", "HI_GK", "question", "answer",
#         lambda r: r.get("lang", "") == "hi"),
#     ("iisc-adlearn/inquisitive-qahindi", "train", "HI_GK",
#         "question", "short_answer", None),

#     ("thepanacealab/cafps", "train", "UPSC", "question", "answer", None),
# ]

# # â”€â”€â”€â”€â”€ 3) Mini template seeds for small tags â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# WHATSAPP_WISHES = [
#     ("Wish Happy Holi in English.", "Happy Holi! May your life be filled with vibrant colors and joy."),
#     ("Wish Happy Diwali in Hindi.", "à¤¦à¤¿à¤µà¤¾à¤²à¥€ à¤•à¥€ à¤¹à¤¾à¤°à¥à¤¦à¤¿à¤• à¤¶à¥à¤­à¤•à¤¾à¤®à¤¨à¤¾à¤à¤! à¤¯à¤¹ à¤ªà¤°à¥à¤µ à¤†à¤ªà¤•à¥‡ à¤œà¥€à¤µà¤¨ à¤®à¥‡à¤‚ à¤–à¥à¤¶à¤¿à¤¯à¤¾à¤ à¤²à¤¾à¤à¥¤"),
#     ("Wish Happy NewÂ Year to a friend.", "Happy NewÂ Year! May our friendship grow stronger in 2026."),
# ]

# YOUTUBE_IDEAS = [
#     "Vlog: Day in the Life of a Commerce Student",
#     "Python Quickâ€‘Tips (60â€‘second shorts)",
#     "Street Food Review in Mumbai",
#     "Budget Travel Series (â‚¹500 Per Day)",
# ]

# STARTUP_AREAS = [
#     "AIâ€‘powered personal finance advisor for tierâ€‘2 cities.",
#     "Hyperlocal grocery delivery on electric cycles.",
#     "Voiceâ€‘first English learning app for Hindi speakers.",
#     "Battery swap network for twoâ€‘wheelers.",
# ]

# EMAIL_TEMPLATES = [
#     ("Apology email to customer for delayed shipment",
#      "Subject: Apology for Delay in Delivery\n\nDear {name},\nWe regret the delay of your order #{id}. â€¦"),
#     ("Leave application for family function",
#      "Subject: Leave Request (25â€“27â€¯July)\n\nDear {manager},\nI would like to apply for leave â€¦"),
# ]

# RESUME_TEMPLATES = [
#     ("Fresher software engineer resume summary",
#      "Enthusiastic Computer Science graduate proficient in Python, React, and SQL â€¦"),
#     ("Cover letter for digital marketing intern",
#      "Dear Hiring Manager,\nI am writing to apply for the Digital Marketing Internship at â€¦"),
# ]

# ENGLISH_DRILLS = [
#     ("Correct this sentence: \"She don't like iceâ€‘cream.\"", "She doesn't like iceâ€‘cream."),
#     ("Translate to English: \"à¤®à¥à¤à¥‡ à¤ªà¤¢à¤¼à¤¨à¤¾ à¤ªà¤¸à¤‚à¤¦ à¤¹à¥ˆà¥¤\"", "I like to read."),
# ]

# # â”€â”€â”€â”€â”€ 4) Output  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# OUT = Path("data/multitask/input.txt")
# OUT.parent.mkdir(parents=True, exist_ok=True)

# # â”€â”€â”€â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# seen_questions: set[str] = set()              # global dedup set
# tag_counts      = defaultdict(int)            # live quota tracker

# def unique(question: str) -> bool:
#     """Return True if question is new; else False (duplicate)."""
#     q = question.strip()
#     if q in seen_questions:
#         return False
#     seen_questions.add(q)
#     return True

# def write_block(file, tag: str, q: str, a: str = ""):
#     file.write(f"[{tag}]\n")
#     if tag == "CODE":
#         file.write(q.strip() + "\n\n")
#     else:
#         file.write(f"Q: {q.strip()}\nA: {a.strip()}\n\n")

# # â”€â”€â”€â”€â”€ 5) Add HuggingFace data (with dedup) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# def load_hf_blocks(out):
#     for ds_id, split, tag, qk, ak, filtr in HF_SOURCES:
#         if tag_counts[tag] >= QUOTA[tag]:
#             continue
#         need = QUOTA[tag] - tag_counts[tag]
#         print(f"ðŸ”¹ HF â†’ {ds_id}:{split}  (need {need})")
#         try:
#             ds = load_dataset(ds_id, split=split)
#         except Exception as e:
#             print(f"   âš ï¸  Skip {ds_id}: {e}")
#             continue

#         # Shuffle once for diversity
#         rows = ds.shuffle(seed=42)

#         for row in rows:
#             if tag_counts[tag] >= QUOTA[tag]:
#                 break
#             if filtr and not filtr(row):
#                 continue

#             q = row[qk]
#             if not unique(q):
#                 continue

#             # Resolve answer
#             a = ""
#             if ak:
#                 ref = row
#                 for key in ak.split("."):
#                     ref = ref[key]
#                 a = ref[0] if isinstance(ref, list) else ref

#             write_block(out, tag, q, a)
#             tag_counts[tag] += 1

# # â”€â”€â”€â”€â”€ 6) Template generator with dedup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# def fill_from_templates(out, tag, pairs):
#     while tag_counts[tag] < QUOTA[tag]:
#         q, a = pairs[tag_counts[tag] % len(pairs)]  # simple cycling
#         # personalise placeholders
#         a_mod = (a.replace("{name}", choice(["Akash", "Sana", "Ravi"]))
#                    .replace("{manager}", choice(["Sir", "Ma'am"]))
#                    .replace("{id}", str(randint(1000, 9999))))
#         if not unique(q):
#             # modify question slightly to remain unique
#             q = q + f" ({randint(1,9999)})"
#         write_block(out, tag, q, a_mod)
#         tag_counts[tag] += 1

# # â”€â”€â”€â”€â”€ 7) Main routine â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# def main():
#     with OUT.open("w", encoding="utf-8") as out:
#         # A) Big HF quotas
#         load_hf_blocks(out)

#         # B) Small template quotas
#         fill_from_templates(out, "WHATSAPP", WHATSAPP_WISHES)
#         fill_from_templates(out, "YOUTUBE",
#             [(f"YouTube channel idea: {idea}", idea) for idea in YOUTUBE_IDEAS])
#         fill_from_templates(out, "STARTUP",
#             [(f"Startup idea using AI: {idea}", idea) for idea in STARTUP_AREAS])
#         fill_from_templates(out, "EMAIL", EMAIL_TEMPLATES)
#         fill_from_templates(out, "RESUME", RESUME_TEMPLATES)
#         fill_from_templates(out, "ENGLISH", ENGLISH_DRILLS)

#     # Summary
#     print("\nâœ… Final counts (deduplicated)")
#     for tg in QUOTA:
#         print(f"  {tg:<10}: {tag_counts[tg]} / {QUOTA[tg]}")
#     print(f"\nðŸš€ TOTAL blocks: {sum(tag_counts.values()):,} (all unique)")

# if __name__ == "__main__":
#     main()





# # #!/usr/bin/env python3
# # # generate_data.py  (with block counter)

# # from datasets import load_dataset
# # from pathlib import Path
# # import csv, pandas as pd

# # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # # HF datasets: (id, split, tag, q_key, a_key, optional_filter_fn)
# # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # HF_DATASETS = [
# #     ("squad", "train",        "EN_GK", "question", "answers.text"),
# #     ("squad", "validation",   "EN_GK", "question", "answers.text"),
# #     ("ai4bharat/indicqa", "train", "HI_GK", "question", "answer",
# #         lambda r: r.get("lang", "") == "hi"),
# #     ("iamtarun/upsc-csat", "train", "UPSC", "question", "correct_answer"),
# #     ("codeparrot/github-code", "train", "CODE", "code", None),
# # ]

# # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # # Kaggle CSV sources (manually downloaded)
# # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # KAGGLE_CSV_FILES = [
# #     ("data/kaggle/resume_samples.csv",  "RESUME"),
# #     ("data/kaggle/youtube_ideas.csv",   "YOUTUBE"),
# #     ("data/kaggle/email_templates.csv", "EMAIL"),
# # ]

# # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # # Manual CSVs (optional)
# # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # MANUAL_DIR = Path("data/manual")

# # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # # Final output
# # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # OUTPUT = Path("data/multitask/input-gemini.txt")
# # OUTPUT.parent.mkdir(parents=True, exist_ok=True)


# # def write(f, tag, q, a=""):
# #     """Write one block and return 1 (so we can sum later)."""
# #     q, a = str(q).strip(), str(a).strip()
# #     if tag == "CODE":
# #         f.write(f"[{tag}]\n{q}\n\n")
# #     else:
# #         f.write(f"[{tag}]\nQ: {q}\nA: {a}\n\n")
# #     return 1


# # def generate(max_per=5000):
# #     total = 0  # â† block counter
# #     with OUTPUT.open("w", encoding="utf-8") as out:

# #         # 1ï¸âƒ£ Hugging Face datasets
# #         for ds_id, split, tag, qk, ak, *filter_fn in HF_DATASETS:
# #             print(f"ðŸ”¹ HF: {ds_id}:{split}")
# #             try:
# #                 ds = load_dataset(ds_id, split=split)
# #             except Exception as e:
# #                 print(f"  âš ï¸ Skipped {ds_id}: {e}")
# #                 continue
# #             filt = filter_fn[0] if filter_fn else None
# #             for i, r in enumerate(ds):
# #                 if max_per and i >= max_per:
# #                     break
# #                 if filt and not filt(r):
# #                     continue
# #                 q = r.get(qk)
# #                 if ak:
# #                     a = r
# #                     for k in ak.split("."):
# #                         a = a[k]
# #                     a = a[0] if isinstance(a, list) else a
# #                     total += write(out, tag, q, a)
# #                 else:
# #                     total += write(out, tag, q)

# #         # 2ï¸âƒ£ Kaggle CSVs
# #         for path, tag in KAGGLE_CSV_FILES:
# #             try:
# #                 df = pd.read_csv(path)
# #                 print(f"ðŸ”¹ Kaggle: {path}")
# #                 for i, row in df.iterrows():
# #                     if max_per and i >= max_per:
# #                         break
# #                     total += write(out, tag, row["question"], row.get("answer", ""))
# #             except Exception as e:
# #                 print(f"  âš ï¸ Skipped Kaggle {path}: {e}")

# #         # 3ï¸âƒ£ Manual CSVs
# #         if MANUAL_DIR.exists():
# #             for file in MANUAL_DIR.glob("*.csv"):
# #                 print(f"ðŸ”¹ Manual: {file.name}")
# #                 with file.open("r", encoding="utf-8") as fcsv:
# #                     reader = csv.DictReader(fcsv)
# #                     for i, row in enumerate(reader):
# #                         if max_per and i >= max_per:
# #                             break
# #                         total += write(out, row["tag"].upper(),
# #                                        row["question"], row.get("answer", ""))

# #     print(f"\nâœ… Dataset written to â†’ {OUTPUT}  |  {total:,} blocks generated")


# # if __name__ == "__main__":
# #     generate(max_per=5000)


# # # # generate_data.py

# # # from datasets import load_dataset
# # # from pathlib import Path
# # # import csv, pandas as pd

# # # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # # # HF datasets: (id, split, tag, q_key, a_key, optional_filter_fn)
# # # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # # HF_DATASETS = [
# # #     ("squad", "train", "EN_GK", "question", "answers.text"),
# # #     ("squad", "validation", "EN_GK", "question", "answers.text"),
# # #     ("ai4bharat/indicqa", "train", "HI_GK", "question", "answer", lambda r: r.get("lang", "") == "hi"),
# # #     ("iamtarun/upsc-csat", "train", "UPSC", "question", "correct_answer"),
# # #     ("codeparrot/github-code", "train", "CODE", "code", None),
# # # ]

# # # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # # # Kaggle CSV sources (manually downloaded)
# # # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # # KAGGLE_CSV_FILES = [
# # #     ("data/kaggle/resume_samples.csv", "RESUME"),    # tag, file_path
# # #     ("data/kaggle/youtube_ideas.csv", "YOUTUBE"),
# # #     ("data/kaggle/email_templates.csv", "EMAIL"),
# # # ]

# # # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # # # Manual CSVs (optional)
# # # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # # MANUAL_DIR = Path("data/manual")  # must contain CSVs with tag,question,answer

# # # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # # # Final output
# # # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # # OUTPUT = Path("data/multitask/input.txt")
# # # OUTPUT.parent.mkdir(parents=True, exist_ok=True)

# # # def write(f, tag, q, a=""):
# # #     q, a = q.strip(), a.strip()
# # #     if tag == "CODE":
# # #         f.write(f"[{tag}]\n{q}\n\n")
# # #     else:
# # #         f.write(f"[{tag}]\nQ: {q}\nA: {a}\n\n")

# # # def generate(max_per=5000):
# # #     with OUTPUT.open("w", encoding="utf-8") as out:

# # #         # 1ï¸âƒ£ Hugging Face datasets
# # #         for ds_id, split, tag, qk, ak, *filter_fn in HF_DATASETS:
# # #             print(f"ðŸ”¹ HF: {ds_id}:{split}")
# # #             try:
# # #                 ds = load_dataset(ds_id, split=split)
# # #             except Exception as e:
# # #                 print(f"âš ï¸ Skipped {ds_id}: {e}")
# # #                 continue
# # #             filt = filter_fn[0] if filter_fn else None
# # #             for i, r in enumerate(ds):
# # #                 if max_per and i >= max_per:
# # #                     break
# # #                 if filt and not filt(r): continue
# # #                 q = r.get(qk)
# # #                 if ak:
# # #                     a = r
# # #                     for k in ak.split("."):
# # #                         a = a[k]
# # #                     a = a[0] if isinstance(a, list) else a
# # #                     write(out, tag, q, a)
# # #                 else:
# # #                     write(out, tag, q)

# # #         # 2ï¸âƒ£ Kaggle CSVs
# # #         for path, tag in KAGGLE_CSV_FILES:
# # #             try:
# # #                 df = pd.read_csv(path)
# # #                 print(f"ðŸ”¹ Kaggle: {path}")
# # #                 for i, row in df.iterrows():
# # #                     if max_per and i >= max_per:
# # #                         break
# # #                     write(out, tag, row["question"], row.get("answer", ""))
# # #             except Exception as e:
# # #                 print(f"âš ï¸ Skipped Kaggle {path}: {e}")

# # #         # 3ï¸âƒ£ Manual CSVs
# # #         if MANUAL_DIR.exists():
# # #             for file in MANUAL_DIR.glob("*.csv"):
# # #                 print(f"ðŸ”¹ Manual: {file.name}")
# # #                 with file.open("r", encoding="utf-8") as fcsv:
# # #                     reader = csv.DictReader(fcsv)
# # #                     for i, row in enumerate(reader):
# # #                         if max_per and i >= max_per:
# # #                             break
# # #                         write(out, row["tag"].upper(), row["question"], row.get("answer", ""))

# # #     print(f"\nâœ… Dataset written to â†’ {OUTPUT}")

# # # # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # # if __name__ == "__main__":
# # #     generate(max_per=5000)
